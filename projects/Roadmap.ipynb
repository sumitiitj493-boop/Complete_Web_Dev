{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¬ EchoVision - Complete Project Blueprint\n",
        "\n",
        "## The Semantic Video Search Engine\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #58a6ff; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h3 style=\"color: #58a6ff; margin-top: 0;\">ðŸ“‹ Project Summary</h3>\n",
        "<p style=\"color: #c9d1d9;\"><strong>Duration:</strong> 4 Weeks | <strong>Daily Commitment:</strong> 1-2 Hours</p>\n",
        "<p style=\"color: #8b949e;\">Build a platform where users upload videos and search inside them using natural language. The player jumps to exact timestamps where concepts are discussed.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ—ï¸ System Architecture\n",
        "\n",
        "```\n",
        "+------------------+     +------------------------+     +------------------+\n",
        "|                  |     |                        |     |                  |\n",
        "|   FRONTEND       |     |      BACKEND           |     |   ML PIPELINE    |\n",
        "|   (React.js)     |     |      (FastAPI)         |     |   (Python)       |\n",
        "|                  |     |                        |     |                  |\n",
        "+--------+---------+     +----------+-------------+     +--------+---------+\n",
        "         |                          |                            |\n",
        "         |    HTTP Requests         |                            |\n",
        "         +------------------------->+                            |\n",
        "                                    |     Triggers Processing    |\n",
        "                                    +--------------------------->+\n",
        "                                    |                            |\n",
        "                                    |                   +--------v--------+\n",
        "                                    |                   |                 |\n",
        "                                    |                   |  WHISPER        |\n",
        "                                    |                   |  (Speech-Text)  |\n",
        "                                    |                   |                 |\n",
        "                                    |                   +--------+--------+\n",
        "                                    |                            |\n",
        "                                    |                   +--------v--------+\n",
        "                                    |                   |                 |\n",
        "                                    |                   |  EMBEDDINGS     |\n",
        "                                    |                   |  (Sentence-TF)  |\n",
        "                                    |                   |                 |\n",
        "                                    |                   +--------+--------+\n",
        "                                    |                            |\n",
        "                                    |                   +--------v--------+\n",
        "                                    |                   |                 |\n",
        "                                    +<------------------+  VECTOR DB      |\n",
        "                                    |   Search Results  |  (ChromaDB)     |\n",
        "                                    |                   |                 |\n",
        "                                    |                   +-----------------+\n",
        "         +<-------------------------+\n",
        "         |   JSON Response\n",
        "         |   (timestamps)\n",
        "         v\n",
        "+--------+---------+\n",
        "|                  |\n",
        "|  VIDEO PLAYER    |\n",
        "|  Seeks to Time   |\n",
        "|                  |\n",
        "+------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Complete Folder Structure\n",
        "\n",
        "```\n",
        "echovision/\n",
        "|\n",
        "+-- .github/\n",
        "|   +-- workflows/\n",
        "|       +-- ci.yml                 # GitHub Actions (optional)\n",
        "|\n",
        "+-- backend/\n",
        "|   +-- app/\n",
        "|   |   +-- __init__.py\n",
        "|   |   +-- main.py                # FastAPI entry point\n",
        "|   |   +-- config.py              # Configuration settings\n",
        "|   |   |\n",
        "|   |   +-- api/\n",
        "|   |   |   +-- __init__.py\n",
        "|   |   |   +-- routes/\n",
        "|   |   |       +-- __init__.py\n",
        "|   |   |       +-- upload.py      # Video upload endpoints\n",
        "|   |   |       +-- search.py      # Search endpoints\n",
        "|   |   |       +-- health.py      # Health check\n",
        "|   |   |\n",
        "|   |   +-- services/\n",
        "|   |   |   +-- __init__.py\n",
        "|   |   |   +-- audio_extractor.py # Extract audio from video\n",
        "|   |   |   +-- transcriber.py     # Whisper transcription\n",
        "|   |   |   +-- embedder.py        # Create embeddings\n",
        "|   |   |   +-- vector_store.py    # ChromaDB operations\n",
        "|   |   |\n",
        "|   |   +-- models/\n",
        "|   |   |   +-- __init__.py\n",
        "|   |   |   +-- schemas.py         # Pydantic models\n",
        "|   |   |\n",
        "|   |   +-- utils/\n",
        "|   |       +-- __init__.py\n",
        "|   |       +-- helpers.py         # Utility functions\n",
        "|   |\n",
        "|   +-- tests/\n",
        "|   |   +-- __init__.py\n",
        "|   |   +-- test_transcriber.py\n",
        "|   |   +-- test_search.py\n",
        "|   |\n",
        "|   +-- uploads/                   # Uploaded videos (gitignore)\n",
        "|   +-- processed/                 # Processed audio files\n",
        "|   +-- chroma_db/                 # Vector database storage\n",
        "|   |\n",
        "|   +-- requirements.txt\n",
        "|   +-- Dockerfile\n",
        "|   +-- .env.example\n",
        "|   +-- .env                       # (gitignore)\n",
        "|\n",
        "+-- frontend/\n",
        "|   +-- public/\n",
        "|   |   +-- index.html\n",
        "|   |   +-- favicon.ico\n",
        "|   |\n",
        "|   +-- src/\n",
        "|   |   +-- components/\n",
        "|   |   |   +-- VideoPlayer.jsx\n",
        "|   |   |   +-- SearchBar.jsx\n",
        "|   |   |   +-- UploadForm.jsx\n",
        "|   |   |   +-- ResultsList.jsx\n",
        "|   |   |   +-- ProgressBar.jsx\n",
        "|   |   |   +-- Navbar.jsx\n",
        "|   |   |\n",
        "|   |   +-- pages/\n",
        "|   |   |   +-- Home.jsx\n",
        "|   |   |   +-- VideoView.jsx\n",
        "|   |   |\n",
        "|   |   +-- services/\n",
        "|   |   |   +-- api.js             # Axios API calls\n",
        "|   |   |\n",
        "|   |   +-- styles/\n",
        "|   |   |   +-- globals.css\n",
        "|   |   |\n",
        "|   |   +-- App.jsx\n",
        "|   |   +-- index.js\n",
        "|   |\n",
        "|   +-- package.json\n",
        "|   +-- tailwind.config.js\n",
        "|   +-- .env.example\n",
        "|\n",
        "+-- notebooks/                     # Jupyter experiments\n",
        "|   +-- 01_whisper_test.ipynb\n",
        "|   +-- 02_embedding_test.ipynb\n",
        "|   +-- 03_chroma_test.ipynb\n",
        "|\n",
        "+-- docs/\n",
        "|   +-- API.md\n",
        "|   +-- SETUP.md\n",
        "|\n",
        "+-- .gitignore\n",
        "+-- README.md\n",
        "+-- docker-compose.yml\n",
        "+-- LICENSE\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âš™ï¸ Pre-Requisites Check\n",
        "\n",
        "Before starting, ensure you have these installed:\n",
        "\n",
        "| Tool | Purpose | Check Command | Install Guide |\n",
        "|------|---------|---------------|---------------|\n",
        "| Python 3.9+ | Backend & ML | `python --version` | python.org |\n",
        "| Node.js 18+ | Frontend | `node --version` | nodejs.org |\n",
        "| Git | Version Control | `git --version` | git-scm.com |\n",
        "| VS Code | Code Editor | - | code.visualstudio.com |\n",
        "| FFmpeg | Audio Processing | `ffmpeg -version` | ffmpeg.org |\n",
        "\n",
        "> ðŸ’¡ **Tip:** FFmpeg is crucial for audio extraction. On Windows, use `winget install ffmpeg` or download from official site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”§ Day 0: Environment Setup (Before Week 1)\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #3fb950; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h3 style=\"color: #3fb950; margin-top: 0;\">Duration: 2 Hours</h3>\n",
        "<p style=\"color: #c9d1d9;\">Set up your complete development environment</p>\n",
        "</div>\n",
        "\n",
        "### Step 1: Create GitHub Repository\n",
        "\n",
        "```bash\n",
        "# Go to github.com and create new repository named \"echovision\"\n",
        "# Select: Add README, Add .gitignore (Python), Add MIT License\n",
        "```\n",
        "\n",
        "### Step 2: Clone and Setup Local Project\n",
        "\n",
        "```bash\n",
        "# Clone your repository\n",
        "git clone https://github.com/YOUR_USERNAME/echovision.git\n",
        "cd echovision\n",
        "\n",
        "# Create folder structure\n",
        "mkdir -p backend/app/api/routes\n",
        "mkdir -p backend/app/services\n",
        "mkdir -p backend/app/models\n",
        "mkdir -p backend/app/utils\n",
        "mkdir -p backend/tests\n",
        "mkdir -p backend/uploads\n",
        "mkdir -p backend/processed\n",
        "mkdir -p frontend/src/components\n",
        "mkdir -p frontend/src/pages\n",
        "mkdir -p frontend/src/services\n",
        "mkdir -p frontend/src/styles\n",
        "mkdir -p notebooks\n",
        "mkdir -p docs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create Python Virtual Environment\n",
        "\n",
        "```bash\n",
        "# Navigate to backend folder\n",
        "cd backend\n",
        "\n",
        "# Create virtual environment\n",
        "python -m venv venv\n",
        "\n",
        "# Activate virtual environment\n",
        "# On Windows:\n",
        "venv\\Scripts\\activate\n",
        "\n",
        "# On Mac/Linux:\n",
        "source venv/bin/activate\n",
        "\n",
        "# You should see (venv) in your terminal now\n",
        "```\n",
        "\n",
        "### Step 4: Create requirements.txt\n",
        "\n",
        "```text\n",
        "# backend/requirements.txt\n",
        "\n",
        "# Web Framework\n",
        "fastapi==0.104.1\n",
        "uvicorn==0.24.0\n",
        "python-multipart==0.0.6\n",
        "\n",
        "# ML Libraries\n",
        "openai-whisper==20231117\n",
        "sentence-transformers==2.2.2\n",
        "torch==2.1.0\n",
        "\n",
        "# Vector Database\n",
        "chromadb==0.4.18\n",
        "\n",
        "# Audio/Video Processing\n",
        "moviepy==1.0.3\n",
        "pydub==0.25.1\n",
        "\n",
        "# Utilities\n",
        "python-dotenv==1.0.0\n",
        "pydantic==2.5.0\n",
        "aiofiles==23.2.1\n",
        "\n",
        "# Testing\n",
        "pytest==7.4.3\n",
        "httpx==0.25.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Install Dependencies\n",
        "\n",
        "```bash\n",
        "# Make sure you're in backend folder with venv activated\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# This will take 10-15 minutes (PyTorch is large)\n",
        "```\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #f0883e; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h4 style=\"color: #f0883e; margin-top: 0;\">Warning: Large Downloads</h4>\n",
        "<p style=\"color: #8b949e;\">PyTorch and Whisper models are large (2-3 GB). Ensure stable internet connection.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Create .gitignore\n",
        "\n",
        "```text\n",
        "# .gitignore (in root folder)\n",
        "\n",
        "# Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "venv/\n",
        ".env\n",
        "*.egg-info/\n",
        "\n",
        "# Uploads and processed files\n",
        "backend/uploads/*\n",
        "backend/processed/*\n",
        "backend/chroma_db/*\n",
        "!backend/uploads/.gitkeep\n",
        "!backend/processed/.gitkeep\n",
        "\n",
        "# ML Models cache\n",
        ".cache/\n",
        "*.pt\n",
        "*.bin\n",
        "\n",
        "# Node.js\n",
        "node_modules/\n",
        "frontend/.env\n",
        "frontend/build/\n",
        "frontend/dist/\n",
        "\n",
        "# IDE\n",
        ".vscode/\n",
        ".idea/\n",
        "\n",
        "# OS\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "\n",
        "# Jupyter\n",
        ".ipynb_checkpoints/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: First Commit\n",
        "\n",
        "```bash\n",
        "# Go back to root folder\n",
        "cd ..\n",
        "\n",
        "# Create placeholder files\n",
        "touch backend/uploads/.gitkeep\n",
        "touch backend/processed/.gitkeep\n",
        "\n",
        "# Add all files\n",
        "git add .\n",
        "\n",
        "# Commit\n",
        "git commit -m \"Initial project setup with folder structure\"\n",
        "\n",
        "# Push to GitHub\n",
        "git push origin main\n",
        "```\n",
        "\n",
        "> ðŸ’¡ **Tip:** Commit after completing each day's task. This builds a good commit history for recruiters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“… WEEK 1: ML Pipeline Foundation\n",
        "\n",
        "<div style=\"background: #0d1117; border: 2px solid #a371f7; padding: 20px; border-radius: 12px; margin: 20px 0;\">\n",
        "<h3 style=\"color: #a371f7; margin-top: 0;\">Week 1 Goal</h3>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Input: Video File (MP4)</p>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Output: Timestamped Transcription (JSON)</p>\n",
        "</div>\n",
        "\n",
        "```\n",
        "+----------------+     +------------------+     +-------------------+\n",
        "|                |     |                  |     |                   |\n",
        "|  VIDEO FILE    +---->+  AUDIO FILE      +---->+  TRANSCRIPTION    |\n",
        "|  (input.mp4)   |     |  (audio.mp3)     |     |  (segments.json)  |\n",
        "|                |     |                  |     |                   |\n",
        "+----------------+     +------------------+     +-------------------+\n",
        "     MoviePy              Whisper Model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 1: Understanding Whisper + First Test\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Learn: What is Whisper? How does it work? |\n",
        "| 30 min | Setup: Test Whisper in Jupyter Notebook |\n",
        "| 30 min | Experiment: Transcribe a short audio file |\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "- OpenAI Whisper GitHub: github.com/openai/whisper\n",
        "- Watch: \"Whisper Explained\" on YouTube (10 min)\n",
        "\n",
        "### Create Test Notebook\n",
        "\n",
        "Create `notebooks/01_whisper_test.ipynb`:\n",
        "\n",
        "```python\n",
        "# Cell 1: Import Whisper\n",
        "import whisper\n",
        "\n",
        "# Cell 2: Load Model (first time downloads ~1GB)\n",
        "model = whisper.load_model(\"base\")  # Options: tiny, base, small, medium, large\n",
        "\n",
        "# Cell 3: Test with a sample audio\n",
        "# Download any short MP3 for testing\n",
        "result = model.transcribe(\"test_audio.mp3\")\n",
        "\n",
        "# Cell 4: Print full text\n",
        "print(result[\"text\"])\n",
        "\n",
        "# Cell 5: Print segments with timestamps (THIS IS CRUCIAL)\n",
        "for segment in result[\"segments\"]:\n",
        "    print(f\"[{segment['start']:.2f}s - {segment['end']:.2f}s]: {segment['text']}\")\n",
        "```\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "```\n",
        "[0.00s - 4.50s]: Hello everyone, welcome to this video.\n",
        "[4.50s - 8.20s]: Today we will learn about machine learning.\n",
        "[8.20s - 12.00s]: Let's start with the basics.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 2: Audio Extraction from Video\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Learn: MoviePy library basics |\n",
        "| 60 min | Code: Build audio_extractor.py service |\n",
        "\n",
        "### Create: `backend/app/services/audio_extractor.py`\n",
        "\n",
        "```python\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "class AudioExtractor:\n",
        "    \"\"\"\n",
        "    Extracts audio from video files.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: str = \"processed\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    def extract(self, video_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract audio from video and save as MP3.\n",
        "        \n",
        "        Args:\n",
        "            video_path: Path to the video file\n",
        "            \n",
        "        Returns:\n",
        "            Path to the extracted audio file\n",
        "        \"\"\"\n",
        "        video_path = Path(video_path)\n",
        "        \n",
        "        # Create output filename\n",
        "        audio_filename = video_path.stem + \".mp3\"\n",
        "        audio_path = self.output_dir / audio_filename\n",
        "        \n",
        "        # Extract audio\n",
        "        video = VideoFileClip(str(video_path))\n",
        "        video.audio.write_audiofile(\n",
        "            str(audio_path),\n",
        "            codec='mp3',\n",
        "            verbose=False,\n",
        "            logger=None\n",
        "        )\n",
        "        video.close()\n",
        "        \n",
        "        return str(audio_path)\n",
        "\n",
        "\n",
        "# Test the extractor\n",
        "if __name__ == \"__main__\":\n",
        "    extractor = AudioExtractor(output_dir=\"../processed\")\n",
        "    audio_path = extractor.extract(\"../uploads/sample.mp4\")\n",
        "    print(f\"Audio saved to: {audio_path}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 3: Build Transcription Service\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 20 min | Learn: Whisper model sizes and tradeoffs |\n",
        "| 70 min | Code: Build transcriber.py service |\n",
        "\n",
        "### Whisper Model Comparison\n",
        "\n",
        "| Model | Parameters | English-Only | Speed | RAM |\n",
        "|-------|------------|--------------|-------|-----|\n",
        "| tiny | 39M | ~10x | Very Fast | ~1GB |\n",
        "| base | 74M | ~7x | Fast | ~1GB |\n",
        "| small | 244M | ~4x | Medium | ~2GB |\n",
        "| medium | 769M | ~2x | Slow | ~5GB |\n",
        "| large | 1550M | 1x | Very Slow | ~10GB |\n",
        "\n",
        "> ðŸ’¡ **Tip:** Use `base` model for development. It balances speed and accuracy.\n",
        "\n",
        "### Create: `backend/app/services/transcriber.py`\n",
        "\n",
        "```python\n",
        "import whisper\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class TranscriptSegment:\n",
        "    \"\"\"Represents a segment of transcribed text with timing.\"\"\"\n",
        "    start: float\n",
        "    end: float\n",
        "    text: str\n",
        "    \n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"start\": self.start,\n",
        "            \"end\": self.end,\n",
        "            \"text\": self.text.strip()\n",
        "        }\n",
        "\n",
        "\n",
        "class Transcriber:\n",
        "    \"\"\"\n",
        "    Transcribes audio files using OpenAI Whisper.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"base\"):\n",
        "        \"\"\"\n",
        "        Initialize the transcriber.\n",
        "        \n",
        "        Args:\n",
        "            model_name: Whisper model size (tiny, base, small, medium, large)\n",
        "        \"\"\"\n",
        "        print(f\"Loading Whisper model: {model_name}...\")\n",
        "        self.model = whisper.load_model(model_name)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    \n",
        "    def transcribe(self, audio_path: str) -> List[TranscriptSegment]:\n",
        "        \"\"\"\n",
        "        Transcribe an audio file.\n",
        "        \n",
        "        Args:\n",
        "            audio_path: Path to the audio file\n",
        "            \n",
        "        Returns:\n",
        "            List of TranscriptSegment objects\n",
        "        \"\"\"\n",
        "        result = self.model.transcribe(\n",
        "            audio_path,\n",
        "            language=\"en\",  # Force English for consistency\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        segments = []\n",
        "        for seg in result[\"segments\"]:\n",
        "            segment = TranscriptSegment(\n",
        "                start=seg[\"start\"],\n",
        "                end=seg[\"end\"],\n",
        "                text=seg[\"text\"]\n",
        "            )\n",
        "            segments.append(segment)\n",
        "        \n",
        "        return segments\n",
        "    \n",
        "    def transcribe_to_json(self, audio_path: str, output_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Transcribe and save as JSON file.\n",
        "        \"\"\"\n",
        "        segments = self.transcribe(audio_path)\n",
        "        \n",
        "        data = {\n",
        "            \"source_file\": audio_path,\n",
        "            \"total_segments\": len(segments),\n",
        "            \"segments\": [seg.to_dict() for seg in segments]\n",
        "        }\n",
        "        \n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        \n",
        "        return output_path\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    transcriber = Transcriber(model_name=\"base\")\n",
        "    segments = transcriber.transcribe(\"../processed/sample.mp3\")\n",
        "    \n",
        "    for seg in segments[:5]:  # Print first 5\n",
        "        print(f\"[{seg.start:.1f}s]: {seg.text}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 4: Create FastAPI Base Structure\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Learn: FastAPI basics (watch tutorial) |\n",
        "| 60 min | Code: Set up FastAPI application structure |\n",
        "\n",
        "### Create: `backend/app/main.py`\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"EchoVision API\",\n",
        "    description=\"Semantic Video Search Engine\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Enable CORS (for frontend communication)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:3000\"],  # React dev server\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"Welcome to EchoVision API\",\n",
        "        \"docs\": \"/docs\"\n",
        "    }\n",
        "\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "```\n",
        "\n",
        "### Run the Server\n",
        "\n",
        "```bash\n",
        "# From backend folder\n",
        "cd backend\n",
        "uvicorn app.main:app --reload\n",
        "\n",
        "# Open browser: http://localhost:8000\n",
        "# API Docs: http://localhost:8000/docs\n",
        "```\n",
        "\n",
        "### Expected Output in Terminal\n",
        "\n",
        "```\n",
        "INFO:     Uvicorn running on http://127.0.0.1:8000\n",
        "INFO:     Started reloader process\n",
        "INFO:     Application startup complete.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 5: Build Upload Endpoint\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 20 min | Learn: File uploads in FastAPI |\n",
        "| 70 min | Code: Create upload route with processing |\n",
        "\n",
        "### Create: `backend/app/api/routes/upload.py`\n",
        "\n",
        "```python\n",
        "from fastapi import APIRouter, UploadFile, File, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "import shutil\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "from app.services.audio_extractor import AudioExtractor\n",
        "from app.services.transcriber import Transcriber\n",
        "\n",
        "router = APIRouter(prefix=\"/api\", tags=[\"Upload\"])\n",
        "\n",
        "# Initialize services (load model once)\n",
        "audio_extractor = AudioExtractor(output_dir=\"processed\")\n",
        "transcriber = Transcriber(model_name=\"base\")\n",
        "\n",
        "UPLOAD_DIR = Path(\"uploads\")\n",
        "UPLOAD_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "ALLOWED_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"}\n",
        "\n",
        "\n",
        "@router.post(\"/upload\")\n",
        "async def upload_video(file: UploadFile = File(...)):\n",
        "    \"\"\"\n",
        "    Upload a video file for processing.\n",
        "    \n",
        "    Returns:\n",
        "        video_id: Unique identifier for the video\n",
        "        segments: Transcribed segments with timestamps\n",
        "    \"\"\"\n",
        "    # Validate file extension\n",
        "    file_ext = Path(file.filename).suffix.lower()\n",
        "    if file_ext not in ALLOWED_EXTENSIONS:\n",
        "        raise HTTPException(\n",
        "            status_code=400,\n",
        "            detail=f\"File type not allowed. Use: {ALLOWED_EXTENSIONS}\"\n",
        "        )\n",
        "    \n",
        "    # Generate unique ID for this video\n",
        "    video_id = str(uuid.uuid4())[:8]\n",
        "    \n",
        "    # Save uploaded file\n",
        "    video_path = UPLOAD_DIR / f\"{video_id}{file_ext}\"\n",
        "    \n",
        "    with open(video_path, \"wb\") as buffer:\n",
        "        shutil.copyfileobj(file.file, buffer)\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Extract audio\n",
        "        audio_path = audio_extractor.extract(str(video_path))\n",
        "        \n",
        "        # Step 2: Transcribe\n",
        "        segments = transcriber.transcribe(audio_path)\n",
        "        \n",
        "        return JSONResponse({\n",
        "            \"success\": True,\n",
        "            \"video_id\": video_id,\n",
        "            \"filename\": file.filename,\n",
        "            \"total_segments\": len(segments),\n",
        "            \"segments\": [seg.to_dict() for seg in segments]\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Update main.py to Include Router\n",
        "\n",
        "```python\n",
        "# backend/app/main.py - Updated\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from app.api.routes import upload  # Add this import\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"EchoVision API\",\n",
        "    description=\"Semantic Video Search Engine\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:3000\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Include routers\n",
        "app.include_router(upload.router)  # Add this line\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"EchoVision API\", \"docs\": \"/docs\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 6: Test Pipeline End-to-End\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Test: Upload video via FastAPI docs |\n",
        "| 30 min | Debug: Fix any errors |\n",
        "| 30 min | Git: Commit Week 1 progress |\n",
        "\n",
        "### Testing with Swagger UI\n",
        "\n",
        "```\n",
        "1. Run server: uvicorn app.main:app --reload\n",
        "2. Open: http://localhost:8000/docs\n",
        "3. Click on POST /api/upload\n",
        "4. Click \"Try it out\"\n",
        "5. Upload a small video file (30 seconds max)\n",
        "6. Click Execute\n",
        "7. Check the response for segments\n",
        "```\n",
        "\n",
        "### Expected Response\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"success\": true,\n",
        "  \"video_id\": \"a1b2c3d4\",\n",
        "  \"filename\": \"lecture.mp4\",\n",
        "  \"total_segments\": 15,\n",
        "  \"segments\": [\n",
        "    {\n",
        "      \"start\": 0.0,\n",
        "      \"end\": 5.2,\n",
        "      \"text\": \"Hello everyone welcome to this lecture.\"\n",
        "    },\n",
        "    {\n",
        "      \"start\": 5.2,\n",
        "      \"end\": 10.8,\n",
        "      \"text\": \"Today we will discuss gradient descent.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 7: Refactor + Documentation\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Refactor: Clean up code, add comments |\n",
        "| 30 min | Document: Update README |\n",
        "| 30 min | Git: Create proper commit history |\n",
        "\n",
        "### Git Commands for Week 1\n",
        "\n",
        "```bash\n",
        "git add .\n",
        "git commit -m \"feat: Add ML pipeline - audio extraction and transcription\"\n",
        "git push origin main\n",
        "```\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #3fb950; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h4 style=\"color: #3fb950; margin-top: 0;\">Week 1 Checkpoint</h4>\n",
        "<p style=\"color: #c9d1d9;\">You should now be able to:</p>\n",
        "<ul style=\"color: #8b949e;\">\n",
        "<li>Upload a video file</li>\n",
        "<li>Extract audio automatically</li>\n",
        "<li>Get transcription with timestamps</li>\n",
        "</ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“… WEEK 2: Vector Search Implementation\n",
        "\n",
        "<div style=\"background: #0d1117; border: 2px solid #58a6ff; padding: 20px; border-radius: 12px; margin: 20px 0;\">\n",
        "<h3 style=\"color: #58a6ff; margin-top: 0;\">Week 2 Goal</h3>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Input: User Query (\"explain gradient descent\")</p>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Output: Matching Timestamps with Confidence Scores</p>\n",
        "</div>\n",
        "\n",
        "```\n",
        "+------------------+     +------------------+     +-------------------+\n",
        "|                  |     |                  |     |                   |\n",
        "|  TEXT SEGMENTS   +---->+  EMBEDDINGS      +---->+  VECTOR DB        |\n",
        "|  (from Week 1)   |     |  (384 dim)       |     |  (ChromaDB)       |\n",
        "|                  |     |                  |     |                   |\n",
        "+------------------+     +------------------+     +--------+----------+\n",
        "                                                          |\n",
        "+------------------+     +------------------+              |\n",
        "|                  |     |                  |              |\n",
        "|  USER QUERY      +---->+  QUERY EMBEDDING +------------->+\n",
        "|  (search text)   |     |  (384 dim)       |   Similarity |\n",
        "|                  |     |                  |   Search     |\n",
        "+------------------+     +------------------+              |\n",
        "                                                          v\n",
        "                                             +------------+----------+\n",
        "                                             |                       |\n",
        "                                             |  MATCHING RESULTS     |\n",
        "                                             |  (timestamps + score) |\n",
        "                                             |                       |\n",
        "                                             +-----------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 8: Understanding Embeddings\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 45 min | Learn: What are embeddings? Vector similarity? |\n",
        "| 45 min | Experiment: Create embeddings in notebook |\n",
        "\n",
        "### What is an Embedding?\n",
        "\n",
        "```\n",
        "Text: \"Machine learning is amazing\"\n",
        "            |\n",
        "            v\n",
        "    +---------------+\n",
        "    |   Embedding   |\n",
        "    |    Model      |\n",
        "    +---------------+\n",
        "            |\n",
        "            v\n",
        "Vector: [0.23, -0.45, 0.12, 0.89, ..., 0.34]  (384 dimensions)\n",
        "```\n",
        "\n",
        "### Create: `notebooks/02_embedding_test.ipynb`\n",
        "\n",
        "```python\n",
        "# Cell 1: Install and import\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cell 2: Load model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Cell 3: Create embeddings\n",
        "sentences = [\n",
        "    \"Machine learning is a type of AI\",\n",
        "    \"Artificial intelligence powers ML\",\n",
        "    \"I like pizza\"\n",
        "]\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "print(f\"Shape: {embeddings.shape}\")  # (3, 384)\n",
        "\n",
        "# Cell 4: Check similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity(embeddings)\n",
        "print(similarity)\n",
        "\n",
        "# Sentences 0 and 1 will have HIGH similarity (~0.8)\n",
        "# Sentence 2 will have LOW similarity with others (~0.1)\n",
        "```\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "```\n",
        "Shape: (3, 384)\n",
        "\n",
        "[[1.00  0.82  0.15]\n",
        " [0.82  1.00  0.12]\n",
        " [0.15  0.12  1.00]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 9: Build Embedder Service\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: Create embedder.py service |\n",
        "\n",
        "### Create: `backend/app/services/embedder.py`\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Embedder:\n",
        "    \"\"\"\n",
        "    Creates vector embeddings from text using Sentence Transformers.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize the embedder.\n",
        "        \n",
        "        Args:\n",
        "            model_name: Name of the sentence-transformer model\n",
        "        \"\"\"\n",
        "        print(f\"Loading embedding model: {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "        print(f\"Model loaded! Embedding dimension: {self.dimension}\")\n",
        "    \n",
        "    def embed(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create embeddings for a list of texts.\n",
        "        \n",
        "        Args:\n",
        "            texts: List of text strings\n",
        "            \n",
        "        Returns:\n",
        "            NumPy array of shape (n_texts, embedding_dim)\n",
        "        \"\"\"\n",
        "        embeddings = self.model.encode(\n",
        "            texts,\n",
        "            show_progress_bar=False,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        return embeddings\n",
        "    \n",
        "    def embed_single(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create embedding for a single text.\n",
        "        \n",
        "        Args:\n",
        "            text: Single text string\n",
        "            \n",
        "        Returns:\n",
        "            NumPy array of shape (embedding_dim,)\n",
        "        \"\"\"\n",
        "        return self.model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    embedder = Embedder()\n",
        "    \n",
        "    texts = [\"Hello world\", \"Machine learning basics\"]\n",
        "    embeddings = embedder.embed(texts)\n",
        "    \n",
        "    print(f\"Shape: {embeddings.shape}\")\n",
        "    print(f\"First embedding (first 5 values): {embeddings[0][:5]}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 10: ChromaDB Setup\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 30 min | Learn: ChromaDB basics |\n",
        "| 60 min | Experiment: Store and retrieve vectors |\n",
        "\n",
        "### Create: `notebooks/03_chroma_test.ipynb`\n",
        "\n",
        "```python\n",
        "# Cell 1: Import ChromaDB\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Cell 2: Create client (persistent storage)\n",
        "client = chromadb.Client(Settings(\n",
        "    chroma_db_impl=\"duckdb+parquet\",\n",
        "    persist_directory=\"./chroma_db\"\n",
        "))\n",
        "\n",
        "# Cell 3: Create or get collection\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"video_segments\",\n",
        "    metadata={\"description\": \"Video transcript segments\"}\n",
        ")\n",
        "\n",
        "# Cell 4: Add documents with metadata\n",
        "collection.add(\n",
        "    documents=[\n",
        "        \"Today we learn about gradient descent\",\n",
        "        \"Neural networks have many layers\",\n",
        "        \"Backpropagation calculates gradients\"\n",
        "    ],\n",
        "    metadatas=[\n",
        "        {\"video_id\": \"abc123\", \"start\": 0.0, \"end\": 5.0},\n",
        "        {\"video_id\": \"abc123\", \"start\": 5.0, \"end\": 10.0},\n",
        "        {\"video_id\": \"abc123\", \"start\": 10.0, \"end\": 15.0}\n",
        "    ],\n",
        "    ids=[\"seg_1\", \"seg_2\", \"seg_3\"]\n",
        ")\n",
        "\n",
        "# Cell 5: Query the collection\n",
        "results = collection.query(\n",
        "    query_texts=[\"explain how gradients work\"],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(results)\n",
        "```\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "```python\n",
        "{\n",
        "    'ids': [['seg_3', 'seg_1']],\n",
        "    'documents': [[\n",
        "        'Backpropagation calculates gradients',\n",
        "        'Today we learn about gradient descent'\n",
        "    ]],\n",
        "    'metadatas': [[\n",
        "        {'video_id': 'abc123', 'start': 10.0, 'end': 15.0},\n",
        "        {'video_id': 'abc123', 'start': 0.0, 'end': 5.0}\n",
        "    ]],\n",
        "    'distances': [[0.45, 0.52]]  # Lower = more similar\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 11: Build Vector Store Service\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: Create vector_store.py with full CRUD |\n",
        "\n",
        "### Create: `backend/app/services/vector_store.py`\n",
        "\n",
        "```python\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"Represents a search result.\"\"\"\n",
        "    text: str\n",
        "    start: float\n",
        "    end: float\n",
        "    score: float  # Confidence score (0-1)\n",
        "    video_id: str\n",
        "    \n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"text\": self.text,\n",
        "            \"start\": self.start,\n",
        "            \"end\": self.end,\n",
        "            \"score\": round(self.score, 2),\n",
        "            \"video_id\": self.video_id\n",
        "        }\n",
        "\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"\n",
        "    Manages vector storage and retrieval using ChromaDB.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, persist_dir: str = \"chroma_db\"):\n",
        "        \"\"\"\n",
        "        Initialize the vector store.\n",
        "        \n",
        "        Args:\n",
        "            persist_dir: Directory to store the database\n",
        "        \"\"\"\n",
        "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=\"video_segments\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
        "        )\n",
        "        print(f\"Vector store initialized. Total documents: {self.collection.count()}\")\n",
        "    \n",
        "    def add_segments(\n",
        "        self,\n",
        "        video_id: str,\n",
        "        segments: List[Dict[str, Any]]\n",
        "    ) -> int:\n",
        "        \"\"\"\n",
        "        Add transcript segments to the vector store.\n",
        "        \n",
        "        Args:\n",
        "            video_id: Unique video identifier\n",
        "            segments: List of {\"text\": str, \"start\": float, \"end\": float}\n",
        "            \n",
        "        Returns:\n",
        "            Number of segments added\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        \n",
        "        for i, seg in enumerate(segments):\n",
        "            documents.append(seg[\"text\"])\n",
        "            metadatas.append({\n",
        "                \"video_id\": video_id,\n",
        "                \"start\": seg[\"start\"],\n",
        "                \"end\": seg[\"end\"]\n",
        "            })\n",
        "            ids.append(f\"{video_id}_seg_{i}\")\n",
        "        \n",
        "        self.collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        \n",
        "        return len(segments)\n",
        "    \n",
        "    def search(\n",
        "        self,\n",
        "        query: str,\n",
        "        video_id: str = None,\n",
        "        n_results: int = 5\n",
        "    ) -> List[SearchResult]:\n",
        "        \"\"\"\n",
        "        Search for relevant segments.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query text\n",
        "            video_id: Optional - filter by specific video\n",
        "            n_results: Number of results to return\n",
        "            \n",
        "        Returns:\n",
        "            List of SearchResult objects\n",
        "        \"\"\"\n",
        "        where_filter = None\n",
        "        if video_id:\n",
        "            where_filter = {\"video_id\": video_id}\n",
        "        \n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results,\n",
        "            where=where_filter,\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "        \n",
        "        search_results = []\n",
        "        \n",
        "        if results[\"ids\"][0]:\n",
        "            for i in range(len(results[\"ids\"][0])):\n",
        "                # Convert distance to similarity score (0-1)\n",
        "                distance = results[\"distances\"][0][i]\n",
        "                score = 1 - (distance / 2)  # Cosine distance to similarity\n",
        "                \n",
        "                search_results.append(SearchResult(\n",
        "                    text=results[\"documents\"][0][i],\n",
        "                    start=results[\"metadatas\"][0][i][\"start\"],\n",
        "                    end=results[\"metadatas\"][0][i][\"end\"],\n",
        "                    score=score,\n",
        "                    video_id=results[\"metadatas\"][0][i][\"video_id\"]\n",
        "                ))\n",
        "        \n",
        "        return search_results\n",
        "    \n",
        "    def delete_video(self, video_id: str) -> bool:\n",
        "        \"\"\"\n",
        "        Delete all segments for a video.\n",
        "        \"\"\"\n",
        "        self.collection.delete(where={\"video_id\": video_id})\n",
        "        return True\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    store = VectorStore(persist_dir=\"../chroma_db\")\n",
        "    \n",
        "    # Add test segments\n",
        "    segments = [\n",
        "        {\"text\": \"Gradient descent optimizes the model\", \"start\": 0, \"end\": 5},\n",
        "        {\"text\": \"Neural networks learn patterns\", \"start\": 5, \"end\": 10}\n",
        "    ]\n",
        "    store.add_segments(\"test_video\", segments)\n",
        "    \n",
        "    # Search\n",
        "    results = store.search(\"how does optimization work\")\n",
        "    for r in results:\n",
        "        print(f\"[{r.score:.2f}] {r.start}s: {r.text}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 12: Build Search API Endpoint\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: Create search route |\n",
        "\n",
        "### Create: `backend/app/api/routes/search.py`\n",
        "\n",
        "```python\n",
        "from fastapi import APIRouter, Query, HTTPException\n",
        "from typing import Optional, List\n",
        "\n",
        "from app.services.vector_store import VectorStore\n",
        "\n",
        "router = APIRouter(prefix=\"/api\", tags=[\"Search\"])\n",
        "\n",
        "# Initialize vector store\n",
        "vector_store = VectorStore(persist_dir=\"chroma_db\")\n",
        "\n",
        "\n",
        "@router.get(\"/search\")\n",
        "async def search_video(\n",
        "    q: str = Query(..., description=\"Search query\", min_length=2),\n",
        "    video_id: Optional[str] = Query(None, description=\"Filter by video ID\"),\n",
        "    limit: int = Query(5, ge=1, le=20, description=\"Number of results\")\n",
        "):\n",
        "    \"\"\"\n",
        "    Search for content within videos.\n",
        "    \n",
        "    Returns timestamps where the query topic is discussed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = vector_store.search(\n",
        "            query=q,\n",
        "            video_id=video_id,\n",
        "            n_results=limit\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"query\": q,\n",
        "            \"total_results\": len(results),\n",
        "            \"results\": [r.to_dict() for r in results]\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "```\n",
        "\n",
        "### Update main.py\n",
        "\n",
        "```python\n",
        "# Add to imports\n",
        "from app.api.routes import upload, search\n",
        "\n",
        "# Add router\n",
        "app.include_router(search.router)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 13: Integrate Upload with Vector Store\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 60 min | Code: Update upload.py to store embeddings |\n",
        "| 30 min | Test: Full pipeline integration |\n",
        "\n",
        "### Update: `backend/app/api/routes/upload.py`\n",
        "\n",
        "```python\n",
        "# Add import at top\n",
        "from app.services.vector_store import VectorStore\n",
        "\n",
        "# Initialize (add after other initializations)\n",
        "vector_store = VectorStore(persist_dir=\"chroma_db\")\n",
        "\n",
        "# Update the upload function - add after transcription\n",
        "@router.post(\"/upload\")\n",
        "async def upload_video(file: UploadFile = File(...)):\n",
        "    # ... existing code ...\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Extract audio\n",
        "        audio_path = audio_extractor.extract(str(video_path))\n",
        "        \n",
        "        # Step 2: Transcribe\n",
        "        segments = transcriber.transcribe(audio_path)\n",
        "        segment_dicts = [seg.to_dict() for seg in segments]\n",
        "        \n",
        "        # Step 3: Store in vector database (NEW!)\n",
        "        vector_store.add_segments(video_id, segment_dicts)\n",
        "        \n",
        "        return JSONResponse({\n",
        "            \"success\": True,\n",
        "            \"video_id\": video_id,\n",
        "            \"filename\": file.filename,\n",
        "            \"total_segments\": len(segments),\n",
        "            \"message\": \"Video processed and indexed for search\",\n",
        "            \"segments\": segment_dicts\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 14: Test + Debug Week 2\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 45 min | Test: Complete flow (upload -> search) |\n",
        "| 45 min | Git: Commit Week 2 progress |\n",
        "\n",
        "### Testing Workflow\n",
        "\n",
        "```\n",
        "1. Start server: uvicorn app.main:app --reload\n",
        "\n",
        "2. Upload video:\n",
        "   POST http://localhost:8000/api/upload\n",
        "   Body: form-data with video file\n",
        "   Note the video_id in response\n",
        "\n",
        "3. Search:\n",
        "   GET http://localhost:8000/api/search?q=gradient descent&video_id=abc123\n",
        "   \n",
        "4. Verify results contain timestamps\n",
        "```\n",
        "\n",
        "### Expected Search Response\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"query\": \"gradient descent\",\n",
        "  \"total_results\": 3,\n",
        "  \"results\": [\n",
        "    {\n",
        "      \"text\": \"Now let me explain gradient descent algorithm\",\n",
        "      \"start\": 45.2,\n",
        "      \"end\": 52.8,\n",
        "      \"score\": 0.92,\n",
        "      \"video_id\": \"abc123\"\n",
        "    },\n",
        "    {\n",
        "      \"text\": \"Gradient descent helps us minimize the loss\",\n",
        "      \"start\": 78.5,\n",
        "      \"end\": 84.1,\n",
        "      \"score\": 0.87,\n",
        "      \"video_id\": \"abc123\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #3fb950; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h4 style=\"color: #3fb950; margin-top: 0;\">Week 2 Checkpoint</h4>\n",
        "<p style=\"color: #c9d1d9;\">You should now be able to:</p>\n",
        "<ul style=\"color: #8b949e;\">\n",
        "<li>Upload video and auto-index it</li>\n",
        "<li>Search using natural language</li>\n",
        "<li>Get timestamps with confidence scores</li>\n",
        "</ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“… WEEK 3: Frontend Development\n",
        "\n",
        "<div style=\"background: #0d1117; border: 2px solid #3fb950; padding: 20px; border-radius: 12px; margin: 20px 0;\">\n",
        "<h3 style=\"color: #3fb950; margin-top: 0;\">Week 3 Goal</h3>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Build a clean React UI that connects to the backend</p>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Video player that jumps to searched timestamps</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 15: React Project Setup\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 60 min | Setup: Create React app with Vite |\n",
        "| 30 min | Install: Tailwind CSS, Axios |\n",
        "\n",
        "### Create React App\n",
        "\n",
        "```bash\n",
        "# From project root\n",
        "cd frontend\n",
        "\n",
        "# Create Vite React project\n",
        "npm create vite@latest . -- --template react\n",
        "\n",
        "# Install dependencies\n",
        "npm install\n",
        "\n",
        "# Install additional packages\n",
        "npm install axios react-router-dom react-player\n",
        "\n",
        "# Install Tailwind CSS\n",
        "npm install -D tailwindcss postcss autoprefixer\n",
        "npx tailwindcss init -p\n",
        "```\n",
        "\n",
        "### Configure Tailwind\n",
        "\n",
        "```javascript\n",
        "// frontend/tailwind.config.js\n",
        "\n",
        "export default {\n",
        "  content: [\n",
        "    \"./index.html\",\n",
        "    \"./src/**/*.{js,ts,jsx,tsx}\",\n",
        "  ],\n",
        "  theme: {\n",
        "    extend: {},\n",
        "  },\n",
        "  plugins: [],\n",
        "}\n",
        "```\n",
        "\n",
        "```css\n",
        "/* frontend/src/styles/globals.css */\n",
        "\n",
        "@tailwind base;\n",
        "@tailwind components;\n",
        "@tailwind utilities;\n",
        "\n",
        "body {\n",
        "  background-color: #0d1117;\n",
        "  color: #c9d1d9;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 16: Build Upload Component\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: UploadForm component with progress |\n",
        "\n",
        "### Create: `frontend/src/components/UploadForm.jsx`\n",
        "\n",
        "```jsx\n",
        "import { useState } from 'react';\n",
        "import axios from 'axios';\n",
        "\n",
        "const API_URL = 'http://localhost:8000/api';\n",
        "\n",
        "function UploadForm({ onUploadComplete }) {\n",
        "  const [file, setFile] = useState(null);\n",
        "  const [uploading, setUploading] = useState(false);\n",
        "  const [progress, setProgress] = useState(0);\n",
        "  const [error, setError] = useState('');\n",
        "\n",
        "  const handleFileChange = (e) => {\n",
        "    const selectedFile = e.target.files[0];\n",
        "    if (selectedFile) {\n",
        "      setFile(selectedFile);\n",
        "      setError('');\n",
        "    }\n",
        "  };\n",
        "\n",
        "  const handleUpload = async () => {\n",
        "    if (!file) {\n",
        "      setError('Please select a video file');\n",
        "      return;\n",
        "    }\n",
        "\n",
        "    setUploading(true);\n",
        "    setProgress(0);\n",
        "\n",
        "    const formData = new FormData();\n",
        "    formData.append('file', file);\n",
        "\n",
        "    try {\n",
        "      const response = await axios.post(`${API_URL}/upload`, formData, {\n",
        "        headers: { 'Content-Type': 'multipart/form-data' },\n",
        "        onUploadProgress: (progressEvent) => {\n",
        "          const percent = Math.round(\n",
        "            (progressEvent.loaded * 100) / progressEvent.total\n",
        "          );\n",
        "          setProgress(percent);\n",
        "        },\n",
        "      });\n",
        "\n",
        "      onUploadComplete(response.data);\n",
        "    } catch (err) {\n",
        "      setError(err.response?.data?.detail || 'Upload failed');\n",
        "    } finally {\n",
        "      setUploading(false);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div className=\"bg-gray-800 rounded-lg p-6 max-w-md mx-auto\">\n",
        "      <h2 className=\"text-xl font-bold mb-4\">Upload Video</h2>\n",
        "      \n",
        "      <input\n",
        "        type=\"file\"\n",
        "        accept=\"video/*\"\n",
        "        onChange={handleFileChange}\n",
        "        className=\"mb-4 w-full\"\n",
        "        disabled={uploading}\n",
        "      />\n",
        "\n",
        "      {file && (\n",
        "        <p className=\"text-sm text-gray-400 mb-4\">\n",
        "          Selected: {file.name}\n",
        "        </p>\n",
        "      )}\n",
        "\n",
        "      {uploading && (\n",
        "        <div className=\"mb-4\">\n",
        "          <div className=\"bg-gray-700 rounded-full h-2\">\n",
        "            <div\n",
        "              className=\"bg-blue-500 h-2 rounded-full transition-all\"\n",
        "              style={{ width: `${progress}%` }}\n",
        "            />\n",
        "          </div>\n",
        "          <p className=\"text-sm mt-2\">Processing... {progress}%</p>\n",
        "        </div>\n",
        "      )}\n",
        "\n",
        "      {error && (\n",
        "        <p className=\"text-red-500 text-sm mb-4\">{error}</p>\n",
        "      )}\n",
        "\n",
        "      <button\n",
        "        onClick={handleUpload}\n",
        "        disabled={uploading || !file}\n",
        "        className=\"w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 \n",
        "                   text-white font-bold py-2 px-4 rounded\"\n",
        "      >\n",
        "        {uploading ? 'Processing...' : 'Upload & Process'}\n",
        "      </button>\n",
        "    </div>\n",
        "  );\n",
        "}\n",
        "\n",
        "export default UploadForm;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 17: Build Search Bar Component\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: SearchBar with debouncing |\n",
        "\n",
        "### Create: `frontend/src/components/SearchBar.jsx`\n",
        "\n",
        "```jsx\n",
        "import { useState } from 'react';\n",
        "import axios from 'axios';\n",
        "\n",
        "const API_URL = 'http://localhost:8000/api';\n",
        "\n",
        "function SearchBar({ videoId, onResultClick }) {\n",
        "  const [query, setQuery] = useState('');\n",
        "  const [results, setResults] = useState([]);\n",
        "  const [loading, setLoading] = useState(false);\n",
        "\n",
        "  const handleSearch = async () => {\n",
        "    if (query.length < 2) return;\n",
        "\n",
        "    setLoading(true);\n",
        "    try {\n",
        "      const response = await axios.get(`${API_URL}/search`, {\n",
        "        params: {\n",
        "          q: query,\n",
        "          video_id: videoId,\n",
        "          limit: 5\n",
        "        }\n",
        "      });\n",
        "      setResults(response.data.results);\n",
        "    } catch (err) {\n",
        "      console.error('Search failed:', err);\n",
        "    } finally {\n",
        "      setLoading(false);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  const formatTime = (seconds) => {\n",
        "    const mins = Math.floor(seconds / 60);\n",
        "    const secs = Math.floor(seconds % 60);\n",
        "    return `${mins}:${secs.toString().padStart(2, '0')}`;\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div className=\"w-full max-w-2xl mx-auto\">\n",
        "      {/* Search Input */}\n",
        "      <div className=\"flex gap-2 mb-4\">\n",
        "        <input\n",
        "          type=\"text\"\n",
        "          value={query}\n",
        "          onChange={(e) => setQuery(e.target.value)}\n",
        "          onKeyPress={(e) => e.key === 'Enter' && handleSearch()}\n",
        "          placeholder=\"Search within video (e.g., 'explain gradient descent')\"\n",
        "          className=\"flex-1 bg-gray-800 border border-gray-600 rounded-lg px-4 py-3\n",
        "                     focus:border-blue-500 focus:outline-none\"\n",
        "        />\n",
        "        <button\n",
        "          onClick={handleSearch}\n",
        "          disabled={loading || query.length < 2}\n",
        "          className=\"bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600\n",
        "                     px-6 py-3 rounded-lg font-bold\"\n",
        "        >\n",
        "          {loading ? '...' : 'Search'}\n",
        "        </button>\n",
        "      </div>\n",
        "\n",
        "      {/* Results */}\n",
        "      {results.length > 0 && (\n",
        "        <div className=\"space-y-2\">\n",
        "          {results.map((result, index) => (\n",
        "            <div\n",
        "              key={index}\n",
        "              onClick={() => onResultClick(result.start)}\n",
        "              className=\"bg-gray-800 p-4 rounded-lg cursor-pointer\n",
        "                         hover:bg-gray-700 transition-colors\"\n",
        "            >\n",
        "              <div className=\"flex justify-between items-start\">\n",
        "                <p className=\"text-sm\">{result.text}</p>\n",
        "                <span className=\"bg-green-600 text-xs px-2 py-1 rounded ml-2\">\n",
        "                  {Math.round(result.score * 100)}%\n",
        "                </span>\n",
        "              </div>\n",
        "              <p className=\"text-blue-400 text-sm mt-2\">\n",
        "                Jump to {formatTime(result.start)}\n",
        "              </p>\n",
        "            </div>\n",
        "          ))}\n",
        "        </div>\n",
        "      )}\n",
        "    </div>\n",
        "  );\n",
        "}\n",
        "\n",
        "export default SearchBar;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 18: Build Video Player Component\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: VideoPlayer with seek functionality |\n",
        "\n",
        "### Create: `frontend/src/components/VideoPlayer.jsx`\n",
        "\n",
        "```jsx\n",
        "import { useRef, useEffect, forwardRef, useImperativeHandle } from 'react';\n",
        "\n",
        "const VideoPlayer = forwardRef(({ src }, ref) => {\n",
        "  const videoRef = useRef(null);\n",
        "\n",
        "  // Expose seekTo method to parent components\n",
        "  useImperativeHandle(ref, () => ({\n",
        "    seekTo: (timeInSeconds) => {\n",
        "      if (videoRef.current) {\n",
        "        videoRef.current.currentTime = timeInSeconds;\n",
        "        videoRef.current.play();\n",
        "      }\n",
        "    },\n",
        "    pause: () => {\n",
        "      if (videoRef.current) {\n",
        "        videoRef.current.pause();\n",
        "      }\n",
        "    }\n",
        "  }));\n",
        "\n",
        "  return (\n",
        "    <div className=\"w-full max-w-4xl mx-auto\">\n",
        "      <video\n",
        "        ref={videoRef}\n",
        "        src={src}\n",
        "        controls\n",
        "        className=\"w-full rounded-lg shadow-lg\"\n",
        "      >\n",
        "        Your browser does not support the video tag.\n",
        "      </video>\n",
        "    </div>\n",
        "  );\n",
        "});\n",
        "\n",
        "VideoPlayer.displayName = 'VideoPlayer';\n",
        "\n",
        "export default VideoPlayer;\n",
        "```\n",
        "\n",
        "> ðŸ’¡ **Key Concept:** Using `forwardRef` and `useImperativeHandle` allows the parent component to call `seekTo()` on the video player."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 19: Integrate All Components\n",
        "\n",
        "| Time | Activity |\n",
        "|------|----------|\n",
        "| 90 min | Code: Main page with all components |\n",
        "\n",
        "### Create: `frontend/src/App.jsx`\n",
        "\n",
        "```jsx\n",
        "import { useState, useRef } from 'react';\n",
        "import UploadForm from './components/UploadForm';\n",
        "import VideoPlayer from './components/VideoPlayer';\n",
        "import SearchBar from './components/SearchBar';\n",
        "import './styles/globals.css';\n",
        "\n",
        "function App() {\n",
        "  const [videoData, setVideoData] = useState(null);\n",
        "  const [videoUrl, setVideoUrl] = useState(null);\n",
        "  const playerRef = useRef(null);\n",
        "\n",
        "  const handleUploadComplete = (data) => {\n",
        "    setVideoData(data);\n",
        "    // Create local URL for the uploaded video\n",
        "    // In production, this would be a server URL\n",
        "    setVideoUrl(`http://localhost:8000/uploads/${data.video_id}.mp4`);\n",
        "  };\n",
        "\n",
        "  const handleSeek = (timestamp) => {\n",
        "    if (playerRef.current) {\n",
        "      playerRef.current.seekTo(timestamp);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div className=\"min-h-screen bg-gray-900 py-8 px-4\">\n",
        "      {/* Header */}\n",
        "      <header className=\"text-center mb-12\">\n",
        "        <h1 className=\"text-4xl font-bold text-white mb-2\">\n",
        "          ðŸŽ¬ EchoVision\n",
        "        </h1>\n",
        "        <p className=\"text-gray-400\">\n",
        "          Semantic Video Search Engine\n",
        "        </p>\n",
        "      </header>\n",
        "\n",
        "      {/* Main Content */}\n",
        "      <main className=\"max-w-6xl mx-auto\">\n",
        "        {!videoData ? (\n",
        "          <UploadForm onUploadComplete={handleUploadComplete} />\n",
        "        ) : (\n",
        "          <div className=\"space-y-8\">\n",
        "            {/* Success Message */}\n",
        "            <div className=\"bg-green-900/50 border border-green-500 \n",
        "                            rounded-lg p-4 text-center\">\n",
        "              <p className=\"text-green-400\">\n",
        "                âœ… Video processed! {videoData.total_segments} segments indexed.\n",
        "              </p>\n",
        "            </div>\n",
        "\n",
        "            {/* Video Player */}\n",
        "            <VideoPlayer ref={playerRef} src={videoUrl} />\n",
        "\n",
        "            {/* Search */}\n",
        "            <SearchBar\n",
        "              videoId={videoData.video_id}\n",
        "              onResultClick={handleSeek}\n",
        "            />\n",
        "          </div>\n",
        "        )}\n",
        "      </main>\n",
        "    </div>\n",
        "  );\n",
        "}\n",
        "\n",
        "export default App;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 20-21: Polish + Bug Fixes\n",
        "\n",
        "| Day | Activity |\n",
        "|-----|----------|\n",
        "| Day 20 | Add loading states, error handling, responsive design |\n",
        "| Day 21 | Test full flow, fix bugs, add video serving endpoint |\n",
        "\n",
        "### Add Video Serving to Backend\n",
        "\n",
        "```python\n",
        "# backend/app/main.py - Add static files\n",
        "\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "\n",
        "# Mount uploads folder for serving videos\n",
        "app.mount(\"/uploads\", StaticFiles(directory=\"uploads\"), name=\"uploads\")\n",
        "```\n",
        "\n",
        "<div style=\"background: #161b22; border-left: 4px solid #3fb950; padding: 16px; border-radius: 8px; margin: 16px 0;\">\n",
        "<h4 style=\"color: #3fb950; margin-top: 0;\">Week 3 Checkpoint</h4>\n",
        "<p style=\"color: #c9d1d9;\">You should now have:</p>\n",
        "<ul style=\"color: #8b949e;\">\n",
        "<li>Working upload interface</li>\n",
        "<li>Video player that displays uploaded videos</li>\n",
        "<li>Search bar that returns results</li>\n",
        "<li>Click-to-seek functionality</li>\n",
        "</ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“… WEEK 4: Deployment & Polish\n",
        "\n",
        "<div style=\"background: #0d1117; border: 2px solid #f0883e; padding: 20px; border-radius: 12px; margin: 20px 0;\">\n",
        "<h3 style=\"color: #f0883e; margin-top: 0;\">Week 4 Goal</h3>\n",
        "<p style=\"color: #c9d1d9; font-size: 18px;\">Docker containerization, documentation, and demo recording</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 22: Create Dockerfile\n",
        "\n",
        "### Create: `backend/Dockerfile`\n",
        "\n",
        "```dockerfile\n",
        "FROM python:3.10-slim\n",
        "\n",
        "# Install system dependencies\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    ffmpeg \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements first (for caching)\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install Python dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application code\n",
        "COPY . .\n",
        "\n",
        "# Create directories\n",
        "RUN mkdir -p uploads processed chroma_db\n",
        "\n",
        "# Expose port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run the application\n",
        "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "```\n",
        "\n",
        "### Create: `docker-compose.yml` (root folder)\n",
        "\n",
        "```yaml\n",
        "version: '3.8'\n",
        "\n",
        "services:\n",
        "  backend:\n",
        "    build: ./backend\n",
        "    ports:\n",
        "      - \"8000:8000\"\n",
        "    volumes:\n",
        "      - ./backend/uploads:/app/uploads\n",
        "      - ./backend/chroma_db:/app/chroma_db\n",
        "    environment:\n",
        "      - WHISPER_MODEL=base\n",
        "\n",
        "  frontend:\n",
        "    build: ./frontend\n",
        "    ports:\n",
        "      - \"3000:3000\"\n",
        "    depends_on:\n",
        "      - backend\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 23-24: README & Documentation\n",
        "\n",
        "### Create Professional README.md\n",
        "\n",
        "```markdown\n",
        "# ðŸŽ¬ EchoVision\n",
        "\n",
        "**Semantic Video Search Engine** - Search inside videos using natural language.\n",
        "\n",
        "![Demo](docs/demo.gif)\n",
        "\n",
        "## Features\n",
        "\n",
        "- ðŸŽ¤ **Speech-to-Text**: Automatic transcription using OpenAI Whisper\n",
        "- ðŸ” **Semantic Search**: Find content by meaning, not just keywords\n",
        "- â±ï¸ **Timestamp Navigation**: Click results to jump to exact moments\n",
        "- ðŸ“Š **Confidence Scores**: See match quality for each result\n",
        "\n",
        "## Tech Stack\n",
        "\n",
        "| Component | Technology |\n",
        "|-----------|------------|\n",
        "| Backend | FastAPI (Python) |\n",
        "| ML | OpenAI Whisper, Sentence Transformers |\n",
        "| Vector DB | ChromaDB |\n",
        "| Frontend | React.js + Tailwind CSS |\n",
        "| Container | Docker |\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "```bash\n",
        "# Clone repository\n",
        "git clone https://github.com/username/echovision.git\n",
        "cd echovision\n",
        "\n",
        "# Run with Docker\n",
        "docker-compose up --build\n",
        "\n",
        "# Open browser\n",
        "# Frontend: http://localhost:3000\n",
        "# API Docs: http://localhost:8000/docs\n",
        "```\n",
        "\n",
        "## Architecture\n",
        "\n",
        "[Include architecture diagram here]\n",
        "\n",
        "## License\n",
        "\n",
        "MIT\n",
        "```"
      ]
    },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 25-26: Record Demo Video (Continued)\n",
        "\n",
        "### Demo Script\n",
        "\n",
        "```\n",
        "1. Show the landing page\n",
        "2. Upload a lecture video (use a 5-10 min YouTube tutorial)\n",
        "3. Show processing progress bar\n",
        "4. Type semantic query: \"explain backpropagation\"\n",
        "5. Click result â†’ video jumps to timestamp\n",
        "6. Show fuzzy search: type \"AI\" when video says \"Machine Learning\"\n",
        "7. End with API docs (localhost:8000/docs)\n",
        "```\n",
        "\n",
        "### Recording Tools\n",
        "\n",
        "| Tool | Platform | Cost |\n",
        "|------|----------|------|\n",
        "| OBS Studio | Windows/Mac/Linux | Free |\n",
        "| Loom | Web | Free (5 min limit) |\n",
        "| ScreenPal | Web | Free |\n",
        "\n",
        "> ðŸ’¡ **Tip:** Keep demo under 3 minutes. Recruiters won't watch longer videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 27: Final Polish & Bug Fixes\n",
        "\n",
        "<div style=\"background:#161b22; border-left:4px solid #3fb950; padding:16px; margin:16px 0; border-radius:8px;\">\n",
        "<h3 style=\"color:#3fb950; margin-top:0;\">âœ… Final Checklist</h3>\n",
        "\n",
        "```\n",
        "â–¡ All API endpoints working\n",
        "â–¡ Error handling for invalid files\n",
        "â–¡ Loading states in frontend\n",
        "â–¡ Mobile responsive design\n",
        "â–¡ README with demo GIF\n",
        "â–¡ .env.example file created\n",
        "â–¡ Docker containers build successfully\n",
        "â–¡ No console errors in browser\n",
        "â–¡ All commits have meaningful messages\n",
        "```\n",
        "</div>\n",
        "\n",
        "### Code Cleanup Tasks\n",
        "\n",
        "| Task | File | Action |\n",
        "|------|------|--------|\n",
        "| Remove print statements | `*.py` | Use `logger` instead |\n",
        "| Delete commented code | All | Clean up |\n",
        "| Add docstrings | Functions | Explain purpose |\n",
        "| Type hints | Python | Add where missing |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Day 28: Deployment & Go Live ðŸš€\n",
        "\n",
        "### Deployment Architecture\n",
        "\n",
        "```\n",
        "+------------------+     +------------------+     +------------------+\n",
        "|                  |     |                  |     |                  |\n",
        "|  Vercel          |---->|  Render/Railway  |---->|  ChromaDB        |\n",
        "|  (Frontend)      |     |  (Backend API)   |     |  (Persisted)     |\n",
        "|                  |     |                  |     |                  |\n",
        "+------------------+     +------------------+     +------------------+\n",
        "     React App              FastAPI + ML            Vector Store\n",
        "```\n",
        "\n",
        "### Step 1: Deploy Backend to Render\n",
        "\n",
        "```bash\n",
        "# Create render.yaml in project root\n",
        "```\n",
        "\n",
        "```yaml\n",
        "# render.yaml\n",
        "services:\n",
        "  - type: web\n",
        "    name: echovision-api\n",
        "    env: docker\n",
        "    plan: free\n",
        "    healthCheckPath: /health\n",
        "```\n",
        "\n",
        "### Step 2: Deploy Frontend to Vercel\n",
        "\n",
        "```bash\n",
        "# In frontend directory\n",
        "npm install -g vercel\n",
        "vercel login\n",
        "vercel --prod\n",
        "```\n",
        "\n",
        "### Step 3: Update Environment Variables\n",
        "\n",
        "```bash\n",
        "# In Vercel Dashboard, add:\n",
        "NEXT_PUBLIC_API_URL=https://echovision-api.onrender.com\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<div style=\"background:#161b22; border-left:4px solid #f0883e; padding:16px; margin:16px 0; border-radius:8px;\">\n",
        "<h3 style=\"color:#f0883e; margin-top:0;\">âš ï¸ Important: Free Tier Limitations</h3>\n",
        "\n",
        "| Platform | Limitation | Solution |\n",
        "|----------|------------|----------|\n",
        "| Render Free | 512MB RAM | Use `whisper-tiny` model |\n",
        "| Render Free | Sleeps after 15 min | Add \"cold start\" warning in UI |\n",
        "| Vercel Free | 100GB bandwidth | Sufficient for demo |\n",
        "\n",
        "**If deployment fails:** Record a high-quality demo video instead. Many recruiters prefer video demos over live sites that might be slow.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Project Complete: What You've Built\n",
        "\n",
        "```\n",
        "+------------------------------------------------------------------+\n",
        "|                                                                  |\n",
        "|   ECHOVISION - Semantic Video Search Engine                      |\n",
        "|                                                                  |\n",
        "|   +------------------+    +------------------+                   |\n",
        "|   |  Upload Video    |--->|  Whisper ASR     |                   |\n",
        "|   +------------------+    +--------+---------+                   |\n",
        "|                                    |                             |\n",
        "|                                    v                             |\n",
        "|   +------------------+    +------------------+                   |\n",
        "|   |  Search Query    |--->|  Vector Embed    |                   |\n",
        "|   +------------------+    +--------+---------+                   |\n",
        "|                                    |                             |\n",
        "|                                    v                             |\n",
        "|   +------------------+    +------------------+                   |\n",
        "|   |  Video Player    |<---|  ChromaDB Match  |                   |\n",
        "|   |  Seeks to Time   |    |  Return Timestamp|                   |\n",
        "|   +------------------+    +------------------+                   |\n",
        "|                                                                  |\n",
        "+------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“‹ Resume Bullet Points\n",
        "\n",
        "<div style=\"background:#0d1117; border:1px solid #3fb950; padding:20px; margin:16px 0; border-radius:8px;\">\n",
        "\n",
        "Copy these for your resume:\n",
        "\n",
        "```\n",
        "EchoVision - Semantic Video Search Engine\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "â€¢ Built a multimodal search engine enabling natural language \n",
        "  queries across video content using OpenAI Whisper and \n",
        "  Sentence Transformers\n",
        "\n",
        "â€¢ Implemented vector similarity search with ChromaDB, \n",
        "  achieving sub-100ms query response times on 1-hour videos\n",
        "\n",
        "â€¢ Developed RESTful API with FastAPI handling video processing \n",
        "  pipeline: audio extraction, transcription, and embedding generation\n",
        "\n",
        "â€¢ Created React frontend with real-time progress tracking \n",
        "  and timestamp-based video navigation\n",
        "\n",
        "â€¢ Containerized application using Docker for consistent \n",
        "  deployment across environments\n",
        "\n",
        "Tech: Python, FastAPI, OpenAI Whisper, ChromaDB, React, Docker\n",
        "```\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ—£ï¸ Interview Talking Points\n",
        "\n",
        "### Common Questions & Answers\n",
        "\n",
        "| Question | Your Answer |\n",
        "|----------|-------------|\n",
        "| Why vector search over keyword search? | \"Vector embeddings capture semantic meaning. If video says 'neural networks' and user searches 'deep learning', keyword search fails but vector similarity finds the match.\" |\n",
        "| Why ChromaDB? | \"Open-source, runs locally without API costs, Python-native, and sufficient for demo scale. In production, I'd consider Pinecone for scalability.\" |\n",
        "| Biggest challenge? | \"Handling long videos efficiently. I solved it by chunking transcripts into 30-second windows with 5-second overlap to avoid cutting off context.\" |\n",
        "| How would you scale this? | \"Add Redis for caching frequent queries, use Celery for async processing, and deploy Whisper on GPU instances.\" |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”® Future Enhancements (Bonus Points)\n",
        "\n",
        "<div style=\"background:#161b22; border-left:4px solid #a371f7; padding:16px; margin:16px 0; border-radius:8px;\">\n",
        "<h3 style=\"color:#a371f7; margin-top:0;\">ðŸš€ If You Have Extra Time</h3>\n",
        "\n",
        "| Feature | Difficulty | Impact |\n",
        "|---------|------------|--------|\n",
        "| Add CLIP for visual search | Hard | Very High |\n",
        "| YouTube URL input | Medium | High |\n",
        "| Multiple language support | Easy | Medium |\n",
        "| Export transcript as SRT | Easy | Medium |\n",
        "| User authentication | Medium | Low |\n",
        "\n",
        "</div>\n",
        "\n",
        "### Visual Search with CLIP (Advanced)\n",
        "\n",
        "```python\n",
        "# Future enhancement: Search by image content\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Extract keyframes every 5 seconds\n",
        "# Embed frames with CLIP\n",
        "# Search: \"person writing on whiteboard\" finds visual matches\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Final Project Structure\n",
        "\n",
        "```\n",
        "echovision/\n",
        "â”œâ”€â”€ backend/\n",
        "â”‚   â”œâ”€â”€ app/\n",
        "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
        "â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app\n",
        "â”‚   â”‚   â”œâ”€â”€ config.py            # Settings\n",
        "â”‚   â”‚   â”œâ”€â”€ routers/\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py\n",
        "â”‚   â”‚   â”‚   â””â”€â”€ search.py\n",
        "â”‚   â”‚   â”œâ”€â”€ services/\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ transcription.py # Whisper logic\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Vector generation\n",
        "â”‚   â”‚   â”‚   â””â”€â”€ vector_store.py  # ChromaDB ops\n",
        "â”‚   â”‚   â””â”€â”€ models/\n",
        "â”‚   â”‚       â””â”€â”€ schemas.py       # Pydantic models\n",
        "â”‚   â”œâ”€â”€ tests/\n",
        "â”‚   â”œâ”€â”€ Dockerfile\n",
        "â”‚   â””â”€â”€ requirements.txt\n",
        "â”œâ”€â”€ frontend/\n",
        "â”‚   â”œâ”€â”€ src/\n",
        "â”‚   â”‚   â”œâ”€â”€ components/\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ VideoPlayer.jsx\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ SearchBar.jsx\n",
        "â”‚   â”‚   â”‚   â”œâ”€â”€ UploadZone.jsx\n",
        "â”‚   â”‚   â”‚   â””â”€â”€ ResultsList.jsx\n",
        "â”‚   â”‚   â”œâ”€â”€ pages/\n",
        "â”‚   â”‚   â”œâ”€â”€ hooks/\n",
        "â”‚   â”‚   â””â”€â”€ App.jsx\n",
        "â”‚   â”œâ”€â”€ package.json\n",
        "â”‚   â””â”€â”€ tailwind.config.js\n",
        "â”œâ”€â”€ docker-compose.yml\n",
        "â”œâ”€â”€ README.md\n",
        "â”œâ”€â”€ .gitignore\n",
        "â”œâ”€â”€ .env.example\n",
        "â””â”€â”€ docs/\n",
        "    â”œâ”€â”€ demo.gif\n",
        "    â””â”€â”€ architecture.png\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<div style=\"background:linear-gradient(135deg, #161b22 0%, #0d1117 100%); border:2px solid #58a6ff; padding:24px; margin:20px 0; border-radius:12px; text-align:center;\">\n",
        "\n",
        "<h1 style=\"color:#58a6ff; margin-top:0;\">ðŸŽ‰ Congratulations!</h1>\n",
        "\n",
        "<p style=\"color:#c9d1d9; font-size:18px;\">You've completed the EchoVision project!</p>\n",
        "\n",
        "<p style=\"color:#8b949e;\">In 28 days, you built:</p>\n",
        "\n",
        "<table style=\"margin:auto; color:#c9d1d9;\">\n",
        "<tr><td>âœ…</td><td>Full-stack web application</td></tr>\n",
        "<tr><td>âœ…</td><td>ML pipeline with Whisper</td></tr>\n",
        "<tr><td>âœ…</td><td>Vector database integration</td></tr>\n",
        "<tr><td>âœ…</td><td>Docker containerization</td></tr>\n",
        "<tr><td>âœ…</td><td>Professional documentation</td></tr>\n",
        "</table>\n",
        "\n",
        "<p style=\"color:#3fb950; font-size:16px; margin-top:20px;\"><strong>This project puts you ahead of 90% of candidates.</strong></p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ž Next Steps After Completion\n",
        "\n",
        "| Action | Platform | Priority |\n",
        "|--------|----------|----------|\n",
        "| Add project to GitHub | github.com | ðŸ”´ High |\n",
        "| Pin repository | GitHub Profile | ðŸ”´ High |\n",
        "| Share on LinkedIn | linkedin.com | ðŸŸ¡ Medium |\n",
        "| Write blog post | Medium/Dev.to | ðŸŸ¢ Low |\n",
        "| Submit to r/SideProject | Reddit | ðŸŸ¢ Low |\n",
        "\n",
        "> ðŸ’¡ **Final Tip:** When sharing on LinkedIn, don't just post the link. Write about what you learned, challenges you faced, and what you'd do differently. Recruiters love seeing your thought process.\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your project! You've got this! ðŸ’ª**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}